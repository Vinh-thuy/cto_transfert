from langchain_openai import ChatOpenAI
from typing_extensions import Literal
from langchain_core.messages import HumanMessage, SystemMessage
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_core.tools import tool

from dotenv import load_dotenv
import os
import logging
import random

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Cr√©er un logger sp√©cifique pour le suivi des outils
tool_logger = logging.getLogger("tool_usage")
tool_logger.setLevel(logging.DEBUG)

from pydantic import BaseModel, Field


# Charger les variables d'environnement
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")



llm = ChatOpenAI(model="gpt-4o-mini", openai_api_key=openai_api_key, temperature=0)

# D√©finition d'un outil pour g√©n√©rer des titres accrocheurs
@tool
def generate_catchy_title(theme: str) -> str:
    """G√©n√®re un titre accrocheur pour une histoire bas√©e sur un th√®me donn√©.
    
    Args:
        theme: Le th√®me principal de l'histoire
        
    Returns:
        Un titre accrocheur pour l'histoire
    """
    tool_logger.debug(f"[TOOL] generate_catchy_title appel√© avec le th√®me: '{theme}'")
    
    # Liste de mod√®les de titres accrocheurs
    title_templates = [
        "Le Secret de {}",
        "Quand {} Rencontre le Destin",
        "Les Myst√®res de {}",
        "{} : Une Histoire Inattendue",
        "Le Dernier {} du Monde",
        "L'Incroyable Aventure de {}",
        "{} : La V√©rit√© Cach√©e",
        "Le Jour o√π {} a Chang√©"
    ]
    
    # S√©lection al√©atoire d'un mod√®le et formatage avec le th√®me
    selected_template = random.choice(title_templates)
    title = selected_template.format(theme.capitalize())
    
    tool_logger.info(f"[TOOL] Titre g√©n√©r√©: '{title}' (mod√®le: '{selected_template}')")
    return title

# Lier l'outil au LLM
llm_with_tools = llm.bind_tools([generate_catchy_title])



# Schema for structured output to use as routing logic
class Route(BaseModel):
    step: Literal["chatbot", "story", "joke"] = Field(
        None, description="The next step in the routing process is to direct the request to 'story', 'joke', or 'chatbot' based on the user's request."
    )


# Augment the LLM with schema for structured output
router = llm.with_structured_output(Route)


# State
class State(TypedDict):
    input: str
    decision: str
    output: str


# Nodes
def llm_call_1(state: State):
    """Write a story with a catchy title"""
    logging.info("=== D√©but du traitement du n≈ìud story (llm_call_1) ===")
    logging.info(f"Entr√©e re√ßue: '{state['input']}'")
    
    # Extraire le th√®me principal de l'entr√©e
    theme = state['input'].split()[-1] if len(state['input'].split()) > 0 else "myst√®re"
    logging.info(f"Th√®me extrait pour le titre: '{theme}'")
    
    # Utiliser l'outil pour g√©n√©rer un titre accrocheur
    logging.info("Appel de l'outil generate_catchy_title...")
    title = generate_catchy_title(theme)
    logging.info(f"Titre g√©n√©r√© avec succ√®s: '{title}'")
    
    # Cr√©er le prompt avec le titre g√©n√©r√©
    system_prompt = "Vous √™tes un √©crivain cr√©atif. √âcrivez une histoire avec de l'humour noir en quelques lignes (2 max) avec le titre suivant et bas√©e sur :"
    full_prompt = f"{system_prompt} '{title}' - Th√®me: {state['input']}"
    logging.debug(f"Prompt complet envoy√© au LLM: '{full_prompt}'")
    
    # Utiliser le LLM avec l'outil li√©
    logging.info("Appel du LLM avec les outils li√©s...")
    result = llm_with_tools.invoke(full_prompt)
    logging.debug(f"R√©ponse brute du LLM: '{result.content}'")
    
    # V√©rifier si le LLM a utilis√© des outils dans sa r√©ponse
    if hasattr(result, 'tool_calls') and result.tool_calls:
        logging.info(f"Le LLM a utilis√© {len(result.tool_calls)} outil(s) dans sa r√©ponse")
        for i, tool_call in enumerate(result.tool_calls):
            logging.info(f"Outil {i+1}: {tool_call['name']} avec arguments: {tool_call['args']}")
    else:
        logging.info("Le LLM n'a pas utilis√© d'outils suppl√©mentaires dans sa r√©ponse")
    
    # Formater la sortie avec le titre
    formatted_output = f"**{title}**\n\n{result.content}"
    logging.info(f"Sortie format√©e avec titre: '{formatted_output[:50]}...'")
    logging.info("=== Fin du traitement du n≈ìud story (llm_call_1) ===")
    
    return {"output": formatted_output}


def joke_intro(state: State):
    """Introduction before joke generation"""
    logging.info("Calling joke_intro node before joke generation.")
    
    # Message d'introduction qui demande √† l'utilisateur le th√®me de la blague
    message = ("üòÑüòÑüòÑ üéØ SUPER! Je vais te raconter une blague! üéØ üòÑüòÑüòÑ\n\n"
              "Sur quel th√®me aimerais-tu que je te raconte une blague? (animaux, travail, informatique, etc.)")
    
    # On retourne √† la fois le message d'introduction et l'entr√©e originale
    logging.info(f"Message d'introduction g√©n√©r√©: {message}")
    
    # Stocker l'entr√©e originale pour le noeud suivant
    return {"output": message, "intro_message": message, "original_input": state["input"], "waiting_for_theme": True}


def llm_call_2(state: State):
    """Write a joke based on the theme provided by the user"""
    logging.info("Calling llm_call_2 for joke generation.")
    
    # R√©cup√©rer l'entr√©e qui contient le th√®me de la blague
    joke_input = state["input"]
    logging.info(f"G√©n√©ration d'une blague avec l'entr√©e: '{joke_input}'")
    
    # Cr√©er un syst√®me prompt pour g√©n√©rer une blague de qualit√©
    system_message = SystemMessage(
        content="Tu es un humoriste professionnel sp√©cialis√© dans les blagues courtes et dr√¥les. "
                "G√©n√®re une blague dr√¥le sur le th√®me demand√©. "
                "La blague doit √™tre adapt√©e √† tous les publics et facile √† comprendre."
    )
    
    # Cr√©er le message utilisateur avec le th√®me
    user_message = HumanMessage(content=joke_input)
    
    # G√©n√©rer la blague avec le contexte appropri√©
    result = llm.invoke([system_message, user_message])
    
    # Formater la sortie
    formatted_output = f"Voici une blague sur ce th√®me:\n\n{result.content}"
    
    # Afficher la sortie pour d√©bogage
    logging.info(f"Blague g√©n√©r√©e: '{formatted_output}'")
    
    # S'assurer que l'output est bien retourn√©
    return {"output": formatted_output}


def llm_call_3(state: State):
    """Write a story"""
    logging.info("Calling llm_call_3 for Chatbot info.")
    system_prompt = "Vous √™tes un chatbot LangGraph. Votre r√¥le est d'interagir avec les utilisateurs en r√©pondant √† leurs questions et en fournissant des informations utiles. Vous pouvez g√©n√©rer des histoires, des po√®mes ou des blagues en fonction des demandes des utilisateurs. R√©pondez de mani√®re claire et concise."
    full_prompt = f"{system_prompt} {state['input']}"
    result = llm.invoke(full_prompt)
    return {"output": result.content}


router_prompt = """
You are a senior specialist responsible for classifying incoming requests and questions. Depending on the nature of the question, you must return a decision that will be used to route the question to the appropriate team or agent.

Here are the three possibilities to consider:

1. If the question is related to stories, return "story".
2. If the question is related to jokes, return "joke".
3. If the question concerns information, question ou use case for the chatbot, return "chatbot".

Your response must be a single word: story, joke, or chatbot.
"""

def llm_call_router(state: State):
    """Route the input to the appropriate node"""
    logging.info("=== D√©but du routage de l'entr√©e ===")
    logging.info(f"Entr√©e √† router: '{state['input']}'")
    decision = router.invoke(
        [
            SystemMessage(
                content=router_prompt
            ),
            HumanMessage(content=state["input"]),
        ]
    )
    logging.info(f"D√©cision de routage: '{decision.step}'")
    
    if decision.step == "story":
        logging.info("La requ√™te sera trait√©e par le n≈ìud story (llm_call_1) qui utilise l'outil generate_catchy_title")
    else:
        logging.info(f"La requ√™te sera trait√©e par le n≈ìud {decision.step}")
    
    logging.info("=== Fin du routage de l'entr√©e ===")
    return {"decision": decision.step}


# Conditional edge function to route to the appropriate node
def route_decision(state: State):
    # Return the node name you want to visit next
    if state["decision"] == "story":
        return "llm_call_1"
    elif state["decision"] == "joke":
        return "joke_intro"
    elif state["decision"] == "chatbot":
        return "llm_call_3"


# Build workflow
router_builder = StateGraph(State)

# Add nodes
router_builder.add_node("llm_call_1", llm_call_1)
router_builder.add_node("joke_intro", joke_intro)
router_builder.add_node("llm_call_2", llm_call_2)
router_builder.add_node("llm_call_3", llm_call_3)
router_builder.add_node("llm_call_router", llm_call_router)

# Add edges to connect nodes
router_builder.add_edge(START, "llm_call_router")
router_builder.add_conditional_edges(
    "llm_call_router",
    route_decision,
    {  # Name returned by route_decision : Name of next node to visit
        "llm_call_1": "llm_call_1",
        "joke_intro": "joke_intro",
        "llm_call_3": "llm_call_3",
    },
)
router_builder.add_edge("llm_call_1", END)

# Modifier le flux pour que joke_intro envoie directement son message au front-end
# et que llm_call_2 soit appel√© s√©par√©ment
router_builder.add_edge("joke_intro", END)

# Ajouter une nouvelle ar√™te pour appeler llm_call_2 apr√®s joke_intro
# (cette ar√™te n'est pas utilis√©e dans le flux normal, mais pourrait √™tre utilis√©e plus tard)
router_builder.add_edge("llm_call_2", END)
router_builder.add_edge("llm_call_3", END)

# Compile workflow
router_workflow = router_builder.compile()

# Cr√©er un workflow s√©par√© pour g√©n√©rer des blagues
joke_builder = StateGraph(State)
joke_builder.add_node("llm_call_2", llm_call_2)
joke_builder.add_edge(START, "llm_call_2")
joke_builder.add_edge("llm_call_2", END)

# Compiler le workflow de blagues
joke_workflow = joke_builder.compile()


# Invoke
# Pour tester l'agent avec l'outil de g√©n√©ration de titres
def test_agent():
    logging.info("\n\n=== D√âBUT DU TEST DE L'AGENT ===")
    
    # Test avec une requ√™te de blague pour tester le nouveau n≈ìud joke_intro
    test_input = "Raconte-moi une blague"
    logging.info(f"Test avec l'entr√©e: '{test_input}'")
    
    state = router_workflow.invoke({"input": test_input})
    
    logging.info(f"Sortie finale: '{state['output']}'")
    logging.info("=== FIN DU TEST DE L'AGENT ===\n\n")
    return state

# Ex√©cuter le test
test_result = test_agent()
print(test_result["output"])
